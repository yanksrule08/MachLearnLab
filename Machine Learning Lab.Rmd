---
title: "Machine Learning Lab"
author: "Brett Moberg"
date: "9/21/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal of this report is to predict the manner in which they did the exercise. The variable "classe" denotes how well the exerise is done; this is the variable we want to predict (dependent variable).

## Preparing The Datasets for Model Use

First we load the caret package. Then we create the training and testing datasets. Several columns are mostly filled with blanks, so we convert these to NA's, and then erase columns with NA's from the training data. We also remove variables that are not numeric (user_name, etc.) Lastly we subset the training data to take out the dependent variable and leave only covariates.

```{r prepare1, results="hide", message=F}
library(caret)
```
``` {r prepare2}
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",header=T)
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",header=T)

training[training == ""] <- NA
sapply(training, function(x) sum(is.na(x)))
```
```{r prepare3}
training <-training[,colSums(is.na(training))<100]
training <- subset(training, select=-c(new_window, cvtd_timestamp, user_name, X, raw_timestamp_part_1, raw_timestamp_part_2, num_window))
training_cov <- subset(training, select=-c(classe))
```

## Model Selection

First we set the seed to be able to recreate the psudo-random numbers used in the modeling process. Then we find all variables in the training data that don't have a covariance of at least 0.5 with any other variable. This leaves us with 6 variables, which we use in our model. 2 models are created; one using Naive Bayes (nb) and one using Gradient Boosting (gbm). We also make use of the train control argument to run each model using cross-validation 10 times.

```{r selection1}
set.seed(100)

M <- abs(cor(training_cov))
diag(M) <- 0
MF5 <- which(M> 0.5, arr.ind=T)
MF5_sub <- MF5[,1]
setdiff(names(training_cov), names(MF5_sub))
```
```{r selection2, results="hide", message=F, warning=F}
train_control <- trainControl(method="cv", number=10)

model_nb <- train(classe ~ roll_arm+yaw_arm+roll_forearm+pitch_forearm+yaw_forearm+total_accel_forearm, 
                  data = training, method = "nb",trControl=train_control)

model_gbm <- train(classe ~ roll_arm+yaw_arm+roll_forearm+pitch_forearm+yaw_forearm+total_accel_forearm, 
                   data = training, method = "gbm",trControl=train_control)
```

## Out of Sample Error Rate

To get an estimation of the out of sample error rate without using the test data, we can look at the accuracies
of the 10 instances of each model from the cross-validation. A good estimation of the out of sample error rate is 1 - average of the 10 accuracies.

```{r error}
model_nb$resample
1 - mean(model_nb$resample$Accuracy)
model_gbm$resample
1 - mean(model_gbm$resample$Accuracy)
```

##Conclusion
The gbm model has a much lower estimated error rate (29.2%). This error rate would lead to an estimated 14 of the 20 cases correct in the testing dataset. To make those predictions, we run this code:
```{r test}
predict(model_gbm, newdata = testing)
```
